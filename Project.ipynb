{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengy\\Anaconda3\\envs\\py3\\lib\\site-packages\\gensim\\utils.py:862: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import emoji\n",
    "from emoji.unicode_codes import UNICODE_EMOJI\n",
    "import unicodedata\n",
    "from unidecode import unidecode\n",
    "from gensim import corpora\n",
    "import gensim.models as gsm\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "<pre>\n",
    "t= pd.read_table('train.txt', names=['index', 'label','text'], header=None, delimiter=\"\\t\", quoting=3,encoding = \"utf-8\")\n",
    "t.to_csv('train.csv', encoding='utf-8',index=False,header=True)\n",
    "\n",
    "t17 = pd.read_table('SemEval2017-task4-test.subtask-A.english.txt', names=['index', 'label','text'], header=None, delimiter=\"\\t\", quoting=3,encoding = \"utf-8\")\n",
    "t17.to_csv('2017test.csv', encoding='utf-8',index=False,header=True)\n",
    "\n",
    "t16 = pd.read_table('twitter-2016test-A.txt', names=['index', 'label','text','nan'], header=None, delimiter=\"\\t\", quoting=3,encoding = \"utf-8\")\n",
    "t16 = t16.drop('nan', axis=1)\n",
    "t16.to_csv('2016test.csv', encoding='utf-8',index=False,header=True)\n",
    "\n",
    "</pre>\n",
    "\n",
    "I save the decoding data and reload then"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t= pd.read_csv('train.csv')\n",
    "t17 =  pd.read_csv('2017test.csv') \n",
    "t16 =  pd.read_csv('2016test.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing\n",
    "take a example\n",
    "<pre>\n",
    "text = t17['text'][12275]\n",
    "text\n",
    "'‚ú®‚úîüòòüòòDefeat The Ghost For Me! üëª üî•üê†John Smithüî±üá®üá¶:Zac Efron & beahttps://t.co/5kR4nbliE9 https://t.co/j8XdkHbi5A'\n",
    "text_to_words1( text )\n",
    "['defeat',\n",
    " 'ghost',\n",
    " 'john',\n",
    " 'smith',\n",
    " 'zac',\n",
    " 'efron',\n",
    " 'beaurl',\n",
    " 'url',\n",
    " 'heavi',\n",
    " 'check',\n",
    " 'mark',\n",
    " 'face',\n",
    " 'throw',\n",
    " 'kiss',\n",
    " 'face',\n",
    " 'throw',\n",
    " 'kiss',\n",
    " 'ghost',\n",
    " 'fire',\n",
    " 'tropic',\n",
    " 'fish',\n",
    " 'trident',\n",
    " 'emblem']\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emoji_pattern = re.compile(u'([\\U00002600-\\U000027BF])|([\\U0001f300-\\U0001f64F])|([\\U0001f680-\\U0001f6FF])')\n",
    "def text_to_words( text ):\n",
    "    #convert emoji into name and append at the end of text\n",
    "    text = str(text)\n",
    "    for char in text:\n",
    "        if char in emoji.UNICODE_EMOJI:\n",
    "            text = text + unicodedata.name(char) + \" \"\n",
    "    #remove emojis from text\n",
    "    text = re.sub(emoji_pattern, '', text)\n",
    "\n",
    "    #remove any url to URL\n",
    "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',text)\n",
    "    #Convert any @Username to \"AT_USER\"\n",
    "    text = re.sub('@[^\\s]+','AT_USER',text)\n",
    "    \n",
    "    #Remove additional white spaces\n",
    "    text = re.sub('[\\s]+', ' ', text)\n",
    "    text = re.sub('[\\n]+', ' ', text)\n",
    "    #Remove not alphanumeric symbols white spaces\n",
    "    text = re.sub(r'[^\\w]', ' ', text)\n",
    "    #Replace #word with word\n",
    "    text = re.sub(r'#([^\\s]+)', r'\\1', text)\n",
    "    #trim\n",
    "    text = text.strip('\\'\"')\n",
    "    \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    tokens = word_tokenize(letters_only.lower())\n",
    "    filtered = [word for word in tokens if word not in stop_words]\n",
    "    stemmed = [stemmer.stem(word) for word in filtered] \n",
    "    \n",
    "    return  stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 10000 of 23963\n",
      "\n",
      "Review 20000 of 23963\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#doing training dataset\n",
    "size = len(t['text'])\n",
    "train = []\n",
    "for i in range(size):\n",
    "    #print states after 10000 reviews\n",
    "    if ((i+1)%10000 == 0):\n",
    "        print(\"Review %d of %d\\n\" % ( i+1, size ) )\n",
    "    train.append( text_to_words( t['text'][i] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save cleaned data\n",
    "output_train = pd.DataFrame({\"index\": t['index'],\n",
    "                             \"label\":t['label'],\n",
    "                      \"text\": train})\n",
    "output_train.to_csv(\"clean_text_train.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 10000 of 20633\n",
      "\n",
      "Review 20000 of 20633\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#doing test16 dataset\n",
    "size = len(t16['text'])\n",
    "test16 = []\n",
    "for i in range(size):\n",
    "    #print states after 10000 reviews\n",
    "    if ((i+1)%10000 == 0):\n",
    "        print(\"Review %d of %d\\n\" % ( i+1, size ) )\n",
    "    test16.append( text_to_words( t16['text'][i] ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_test16 = pd.DataFrame({\"index\": t16['index'],\n",
    "                             \"label\":t16['label'],\n",
    "                      \"text\": test16})\n",
    "output_test16.to_csv(\"clean_text_test16.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 10000 of 12284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#doing test17 dataset\n",
    "size = len(t17['text'])\n",
    "test17 = []\n",
    "for i in range(size):\n",
    "    #print states after 10000 reviews\n",
    "    if ((i+1)%10000 == 0):\n",
    "        print(\"Review %d of %d\\n\" % ( i+1, size ) )\n",
    "    test17.append( text_to_words( t17['text'][i] ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_test17 = pd.DataFrame({\"index\": t17['index'],\n",
    "                             \"label\":t17['label'],\n",
    "                      \"text\": test17})\n",
    "output_test17.to_csv(\"clean_text_test17.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "Now datasets are all cleaned, tokenized, and stemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bags of words(Trivial one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_corpus = []\n",
    "for text in train:\n",
    "    train_corpus.append(\" \".join( text))\n",
    "    \n",
    "test16_corpus = []\n",
    "for text in test16:\n",
    "    test16_corpus.append(\" \".join( text))\n",
    "    \n",
    "test17_corpus = []\n",
    "for text in test17:\n",
    "    test17_corpus.append(\" \".join( text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#use the 3000 most frequent word only\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 3000)\n",
    "\n",
    "train_bow = vectorizer.fit_transform(train_corpus)\n",
    "train_bow = train_bow.toarray()\n",
    "\n",
    "test16_bow = vectorizer.transform(test16_corpus)\n",
    "test16_bow = test16_bow.toarray()\n",
    "\n",
    "test17_bow = vectorizer.transform(test17_corpus)\n",
    "test17_bow = test17_bow.toarray()\n",
    "\n",
    "# re-weight the count features into floating point values suitable for usage by a classifier\n",
    "transformer = TfidfTransformer(smooth_idf=False)\n",
    "\n",
    "train_tfidf = transformer.fit_transform(train_bow)\n",
    "train_bow_features1 = train_tfidf.toarray()    \n",
    "\n",
    "test16_tfidf = transformer.transform(test16_bow)\n",
    "test16_bow_features1 = test16_tfidf.toarray()\n",
    "\n",
    "test17_tfidf = transformer.transform(test17_bow)\n",
    "test17_bow_features1 = test17_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I obtain my first features for train, test16, and test17.\n",
    "They are train_bow_features1, test16_bow_features1, test17_bow_features1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without counting first, I use all cleaned data to get second feature\n",
    "\n",
    "<pre>\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "train_bow = vectorizer.fit_transform(train_corpus)\n",
    "train_bow = train_bow.toarray()\n",
    "\n",
    "test16_bow = vectorizer.transform(test16_corpus)\n",
    "test16_bow = test16_bow.toarray()\n",
    "\n",
    "test17_bow = vectorizer.transform(test17_corpus)\n",
    "test17_bow = test17_bow.toarray()\n",
    "\n",
    "</pre>\n",
    "\n",
    "I met memory error, so I didn't use this one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using Word2Vec twitter with dimension 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('word2vec_twitter_model.bin', binary=True,unicode_errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeFeatureVec_w2v(words):\n",
    "\n",
    "    featureVec = np.zeros((400,),dtype ='float32')    \n",
    "    nwords = 0\n",
    "    \n",
    "    for word in words:\n",
    "        if word in model.vocab:\n",
    "            embedding_vector = model[word]\n",
    "            if embedding_vector is not None: \n",
    "                nwords = nwords + 1\n",
    "                featureVec = np.add(featureVec,embedding_vector)\n",
    "    # \n",
    "    # Divide the result by the number of words to get the average\n",
    "    if nwords != 0:\n",
    "        featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_feature_train = []\n",
    "size = len(train)\n",
    "for i in range(size):\n",
    "    w2v_feature_train.append(makeFeatureVec_w2v(train[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_feature_train = np.asarray(w2v_feature_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_feature_test16= []\n",
    "size = len(test16)\n",
    "for i in range(size):\n",
    "    w2v_feature_test16.append(makeFeatureVec_w2v(test16[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_feature_test16 = np.asarray(w2v_feature_test16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_feature_test17= []\n",
    "size = len(test17)\n",
    "for i in range(size):\n",
    "    w2v_feature_test17.append(makeFeatureVec_w2v(test17[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_feature_test17 = np.asarray(w2v_feature_test17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using GloVe glove.twitter.27B.200d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1193514it [01:15, 15710.51it/s]\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open('glove.twitter.27B.200d.txt', encoding=\"utf8\")\n",
    "for line in tqdm(f):\n",
    "    values = line.split(' ')\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:],dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeFeatureVec(words):\n",
    "\n",
    "    featureVec = np.zeros((200,),dtype ='float32')    \n",
    "    nwords = 0\n",
    "    # \n",
    "    #\n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: \n",
    "            nwords = nwords + 1\n",
    "            featureVec = np.add(featureVec,embedding_vector)\n",
    "    # \n",
    "    # Divide the result by the number of words to get the average\n",
    "    if nwords != 0:\n",
    "        featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_feature_train = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = len(train)\n",
    "for i in range(size):\n",
    "    glove_feature_train.append(makeFeatureVec(train[i]))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_feature_train = np.asarray(glove_feature_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_feature_test16= []\n",
    "size = len(test16)\n",
    "for i in range(size):\n",
    "    glove_feature_test16.append(makeFeatureVec(test16[i]))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glove_feature_test16 = np.asarray(glove_feature_test16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_feature_test17= []\n",
    "size = len(test17)\n",
    "for i in range(size):\n",
    "    glove_feature_test17.append(makeFeatureVec(test17[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glove_feature_test17 = np.asarray(glove_feature_test17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ytrain=np.zeros(len(train))\n",
    "ytest16 = np.zeros(len(test16))\n",
    "ytest17 = np.zeros(len(test17))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train)):\n",
    "    if t['label'][i] == 'negative':\n",
    "        ytrain[i] = 0\n",
    "    elif t['label'][i] == 'neutral':\n",
    "        ytrain[i] = 1\n",
    "    elif t['label'][i] == 'positive':\n",
    "        ytrain[i] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test16)):\n",
    "    if t16['label'][i] == 'negative':\n",
    "        ytest16[i] = 0\n",
    "    elif t16['label'][i] == 'neutral':\n",
    "        ytest16[i] = 1\n",
    "    elif t16['label'][i] == 'positive':\n",
    "        ytest16[i] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test17)):\n",
    "    if t17['label'][i] == 'negative':\n",
    "        ytest17[i] = 0\n",
    "    elif t17['label'][i] == 'neutral':\n",
    "        ytest17[i] = 1\n",
    "    elif t16['label'][i] == 'positive':\n",
    "        ytest17[i] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model, datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GloVe word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict 2017 test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logreg_glove = linear_model.LogisticRegression(C=1).fit(glove_feature_train,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score is:  0.547744689637\n",
      "precision score is:  0.570291041865\n",
      "recall score is:  0.547254854431\n"
     ]
    }
   ],
   "source": [
    "log_glove_pre16 = logreg_glove.predict(glove_feature_test16)\n",
    "print(\"F1 score is: \",f1_score(ytest16, log_glove_pre16, average=\"macro\"))\n",
    "print(\"precision score is: \", precision_score(ytest16, log_glove_pre16, average=\"macro\"))\n",
    "print(\"recall score is: \", recall_score(ytest16, log_glove_pre16, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score is:  0.449391040891\n",
      "precision score is:  0.489862971036\n",
      "recall score is:  0.544328109444\n"
     ]
    }
   ],
   "source": [
    "log_glove_pre17 = logreg_glove.predict(glove_feature_test17)\n",
    "print(\"F1 score is: \",f1_score(ytest17, log_glove_pre17, average=\"macro\"))\n",
    "print(\"precision score is: \", precision_score(ytest17, log_glove_pre17, average=\"macro\"))\n",
    "print(\"recall score is: \", recall_score(ytest17, log_glove_pre17, average=\"macro\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c =10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logreg_glove = linear_model.LogisticRegression(C=10).fit(glove_feature_train,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score is:  0.548007772275\n",
      "precision score is:  0.569747516317\n",
      "recall score is:  0.547775013079\n"
     ]
    }
   ],
   "source": [
    "log_glove_pre16 = logreg_glove.predict(glove_feature_test16)\n",
    "print(\"F1 score is: \",f1_score(ytest16, log_glove_pre16, average=\"macro\"))\n",
    "print(\"precision score is: \", precision_score(ytest16, log_glove_pre16, average=\"macro\"))\n",
    "print(\"recall score is: \", recall_score(ytest16, log_glove_pre16, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score is:  0.448200037835\n",
      "precision score is:  0.48831388166\n",
      "recall score is:  0.543157523701\n"
     ]
    }
   ],
   "source": [
    "log_glove_pre17 = logreg_glove.predict(glove_feature_test17)\n",
    "print(\"F1 score is: \",f1_score(ytest17, log_glove_pre17, average=\"macro\"))\n",
    "print(\"precision score is: \", precision_score(ytest17, log_glove_pre17, average=\"macro\"))\n",
    "print(\"recall score is: \", recall_score(ytest17, log_glove_pre17, average=\"macro\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svc_glove = OneVsOneClassifier(LinearSVC(C=1)).fit(glove_feature_train,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score is:  0.556289406102\n",
      "precision score is:  0.570053330053\n",
      "recall score is:  0.556032847424\n"
     ]
    }
   ],
   "source": [
    "svc_glove_pre16 = svc_glove.predict(glove_feature_test16)\n",
    "print(\"F1 score is: \",f1_score(ytest16, svc_glove_pre16, average=\"macro\"))\n",
    "print(\"precision score is: \", precision_score(ytest16, svc_glove_pre16, average=\"macro\"))\n",
    "print(\"recall score is: \", recall_score(ytest16, svc_glove_pre16, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score is:  0.458196656469\n",
      "precision score is:  0.487107591202\n",
      "recall score is:  0.546861783784\n"
     ]
    }
   ],
   "source": [
    "svc_glove_pre17 = svc_glove.predict(glove_feature_test17)\n",
    "print(\"F1 score is: \",f1_score(ytest17, svc_glove_pre17, average=\"macro\"))\n",
    "print(\"precision score is: \", precision_score(ytest17, svc_glove_pre17, average=\"macro\"))\n",
    "print(\"recall score is: \", recall_score(ytest17, svc_glove_pre17, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c =0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svc_glove = OneVsOneClassifier(LinearSVC(C=0.01)).fit(glove_feature_train,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score is:  0.551254461045\n",
      "precision score is:  0.576955384183\n",
      "recall score is:  0.547893613295\n"
     ]
    }
   ],
   "source": [
    "svc_glove_pre16 = svc_glove.predict(glove_feature_test16)\n",
    "print(\"F1 score is: \",f1_score(ytest16, svc_glove_pre16, average=\"macro\"))\n",
    "print(\"precision score is: \", precision_score(ytest16, svc_glove_pre16, average=\"macro\"))\n",
    "print(\"recall score is: \", recall_score(ytest16, svc_glove_pre16, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score is:  0.45612917438\n",
      "precision score is:  0.487830825354\n",
      "recall score is:  0.544817749182\n"
     ]
    }
   ],
   "source": [
    "svc_glove_pre17 = svc_glove.predict(glove_feature_test17)\n",
    "print(\"F1 score is: \",f1_score(ytest17, svc_glove_pre17, average=\"macro\"))\n",
    "print(\"precision score is: \", precision_score(ytest17, svc_glove_pre17, average=\"macro\"))\n",
    "print(\"recall score is: \", recall_score(ytest17, svc_glove_pre17, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svc_glove = OneVsOneClassifier(LinearSVC(C=100)).fit(glove_feature_train,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score is:  0.493904495029\n",
      "precision score is:  0.535331465036\n",
      "recall score is:  0.519237192958\n"
     ]
    }
   ],
   "source": [
    "svc_glove_pre16 = svc_glove.predict(glove_feature_test16)\n",
    "print(\"F1 score is: \",f1_score(ytest16, svc_glove_pre16, average=\"macro\"))\n",
    "print(\"precision score is: \", precision_score(ytest16, svc_glove_pre16, average=\"macro\"))\n",
    "print(\"recall score is: \", recall_score(ytest16, svc_glove_pre16, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score is:  0.473071584131\n",
      "precision score is:  0.467395308476\n",
      "recall score is:  0.493054178442\n"
     ]
    }
   ],
   "source": [
    "svc_glove_pre17 = svc_glove.predict(glove_feature_test17)\n",
    "print(\"F1 score is: \",f1_score(ytest17, svc_glove_pre17, average=\"macro\"))\n",
    "print(\"precision score is: \", precision_score(ytest17, svc_glove_pre17, average=\"macro\"))\n",
    "print(\"recall score is: \", recall_score(ytest17, svc_glove_pre17, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_glove = GaussianNB().fit(glove_feature_train,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score is:  0.493904495029\n",
      "precision score is:  0.535331465036\n",
      "recall score is:  0.519237192958\n"
     ]
    }
   ],
   "source": [
    "nb_glove_pre16 = svc_glove.predict(glove_feature_test16)\n",
    "print(\"F1 score is: \",f1_score(ytest16, nb_glove_pre16, average=\"macro\"))\n",
    "print(\"precision score is: \", precision_score(ytest16, nb_glove_pre16, average=\"macro\"))\n",
    "print(\"recall score is: \", recall_score(ytest16, nb_glove_pre16, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score is:  0.473071584131\n",
      "precision score is:  0.467395308476\n",
      "recall score is:  0.493054178442\n"
     ]
    }
   ],
   "source": [
    "nb_glove_pre17 = svc_glove.predict(glove_feature_test17)\n",
    "print(\"F1 score is: \",f1_score(ytest17, nb_glove_pre17, average=\"macro\"))\n",
    "print(\"precision score is: \", precision_score(ytest17, nb_glove_pre17, average=\"macro\"))\n",
    "print(\"recall score is: \", recall_score(ytest17, nb_glove_pre17, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logreg_w2v = linear_model.LogisticRegression(C=1).fit(w2v_feature_train,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score is:  0.561092233857\n",
      "precision score is:  0.57874658414\n",
      "recall score is:  0.558135880773\n"
     ]
    }
   ],
   "source": [
    "log_w2v_pre16 = logreg_w2v.predict(w2v_feature_test16)\n",
    "print(\"F1 score is: \",f1_score(ytest16, log_w2v_pre16, average=\"macro\"))\n",
    "print(\"precision score is: \", precision_score(ytest16, log_w2v_pre16, average=\"macro\"))\n",
    "print(\"recall score is: \", recall_score(ytest16, log_w2v_pre16, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score is:  0.460666060964\n",
      "precision score is:  0.4960497611\n",
      "recall score is:  0.554125898117\n"
     ]
    }
   ],
   "source": [
    "log_w2v_pre17 = logreg_w2v.predict(w2v_feature_test17)\n",
    "print(\"F1 score is: \",f1_score(ytest17, log_w2v_pre17, average=\"macro\"))\n",
    "print(\"precision score is: \", precision_score(ytest17, log_w2v_pre17, average=\"macro\"))\n",
    "print(\"recall score is: \", recall_score(ytest17, log_w2v_pre17, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_w2v = OneVsOneClassifier(LinearSVC(C=1)).fit(w2v_feature_train,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score is:  0.567886471015\n",
      "precision score is:  0.577526780096\n",
      "recall score is:  0.566561096887\n"
     ]
    }
   ],
   "source": [
    "svc_w2v_pre16 = svc_w2v.predict(w2v_feature_test16)\n",
    "print(\"F1 score is: \",f1_score(ytest16, svc_w2v_pre16, average=\"macro\"))\n",
    "print(\"precision score is: \", precision_score(ytest16, svc_w2v_pre16, average=\"macro\"))\n",
    "print(\"recall score is: \", recall_score(ytest16, svc_w2v_pre16, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score is:  0.469403904932\n",
      "precision score is:  0.493314174559\n",
      "recall score is:  0.554983842779\n"
     ]
    }
   ],
   "source": [
    "svc_w2v_pre17 = svc_w2v.predict(w2v_feature_test17)\n",
    "print(\"F1 score is: \",f1_score(ytest17, svc_w2v_pre17, average=\"macro\"))\n",
    "print(\"precision score is: \", precision_score(ytest17, svc_w2v_pre17, average=\"macro\"))\n",
    "print(\"recall score is: \", recall_score(ytest17, svc_w2v_pre17, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_w2v = GaussianNB().fit(w2v_feature_train,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score is:  0.497824318389\n",
      "precision score is:  0.510449342642\n",
      "recall score is:  0.537545217423\n"
     ]
    }
   ],
   "source": [
    "nb_w2v_pre16 = nb_w2v.predict(w2v_feature_test16)\n",
    "print(\"F1 score is: \",f1_score(ytest16, nb_w2v_pre16, average=\"macro\"))\n",
    "print(\"precision score is: \", precision_score(ytest16, nb_w2v_pre16, average=\"macro\"))\n",
    "print(\"recall score is: \", recall_score(ytest16, nb_w2v_pre16, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score is:  0.466410262051\n",
      "precision score is:  0.479074309766\n",
      "recall score is:  0.479310439926\n"
     ]
    }
   ],
   "source": [
    "nb_w2v_pre17 = nb_w2v.predict(w2v_feature_test17)\n",
    "print(\"F1 score is: \",f1_score(ytest17, nb_w2v_pre17, average=\"macro\"))\n",
    "print(\"precision score is: \", precision_score(ytest17, nb_w2v_pre17, average=\"macro\"))\n",
    "print(\"recall score is: \", recall_score(ytest17, nb_w2v_pre17, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bags of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logreg_bow = linear_model.LogisticRegression(C=1).fit(train_bow_features1,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score is:  0.548461647842\n",
      "precision score is:  0.58781725768\n",
      "recall score is:  0.53658485928\n"
     ]
    }
   ],
   "source": [
    "log_bow_pre16 = logreg_bow.predict(test16_bow_features1)\n",
    "print(\"F1 score is: \",f1_score(ytest16, log_bow_pre16, average=\"macro\"))\n",
    "print(\"precision score is: \", precision_score(ytest16, log_bow_pre16, average=\"macro\"))\n",
    "print(\"recall score is: \", recall_score(ytest16, log_bow_pre16, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score is:  0.405522596993\n",
      "precision score is:  0.48364451733\n",
      "recall score is:  0.505526706393\n"
     ]
    }
   ],
   "source": [
    "log_bow_pre17 = logreg_bow.predict(test17_bow_features1)\n",
    "print(\"F1 score is: \",f1_score(ytest17, log_bow_pre17, average=\"macro\"))\n",
    "print(\"precision score is: \", precision_score(ytest17, log_bow_pre17, average=\"macro\"))\n",
    "print(\"recall score is: \", recall_score(ytest17, log_bow_pre17, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svc_bow = OneVsOneClassifier(LinearSVC(C=1)).fit(train_bow_features1,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score is:  0.55524716356\n",
      "precision score is:  0.558735763077\n",
      "recall score is:  0.552638142701\n"
     ]
    }
   ],
   "source": [
    "svc_bow_pre16 = svc_bow.predict(test16_bow_features1)\n",
    "print(\"F1 score is: \",f1_score(ytest16, svc_bow_pre16, average=\"macro\"))\n",
    "print(\"precision score is: \", precision_score(ytest16, svc_bow_pre16, average=\"macro\"))\n",
    "print(\"recall score is: \", recall_score(ytest16, svc_bow_pre16, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score is:  0.419415977884\n",
      "precision score is:  0.456890269012\n",
      "recall score is:  0.501778551342\n"
     ]
    }
   ],
   "source": [
    "svc_bow_pre17 = svc_bow.predict(test17_bow_features1)\n",
    "print(\"F1 score is: \",f1_score(ytest17, svc_bow_pre17, average=\"macro\"))\n",
    "print(\"precision score is: \", precision_score(ytest17, svc_bow_pre17, average=\"macro\"))\n",
    "print(\"recall score is: \", recall_score(ytest17, svc_bow_pre17, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_bow = GaussianNB().fit(train_bow_features1,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score is:  0.240286324466\n",
      "precision score is:  0.419766717532\n",
      "recall score is:  0.379045652582\n"
     ]
    }
   ],
   "source": [
    "nb_bow_pre16 = nb_bow.predict(test16_bow_features1)\n",
    "print(\"F1 score is: \",f1_score(ytest16, nb_bow_pre16, average=\"macro\"))\n",
    "print(\"precision score is: \", precision_score(ytest16, nb_bow_pre16, average=\"macro\"))\n",
    "print(\"recall score is: \", recall_score(ytest16, nb_bow_pre16, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score is:  0.310988266061\n",
      "precision score is:  0.397494022738\n",
      "recall score is:  0.375633878463\n"
     ]
    }
   ],
   "source": [
    "nb_bow_pre17 = nb_bow.predict(test17_bow_features1)\n",
    "print(\"F1 score is: \",f1_score(ytest17, nb_bow_pre17, average=\"macro\"))\n",
    "print(\"precision score is: \", precision_score(ytest17, nb_bow_pre17, average=\"macro\"))\n",
    "print(\"recall score is: \", recall_score(ytest17, nb_bow_pre17, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Majority Vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ypre16 = []\n",
    "ypre16.append(log_glove_pre16)\n",
    "ypre16.append(svc_glove_pre16)\n",
    "ypre16.append(nb_glove_pre16)\n",
    "ypre16.append(log_w2v_pre16)\n",
    "ypre16.append(svc_w2v_pre16)\n",
    "ypre16.append(nb_w2v_pre16)\n",
    "ypre16.append(log_bow_pre16)\n",
    "ypre16.append(svc_bow_pre16)\n",
    "ypre16.append(nb_bow_pre16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ypre16 = np.asarray(ypre16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "majorityvote16 = np.zeros(ypre16.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(ypre16.shape[1]):\n",
    "    vote = np.zeros(3)\n",
    "    for j in range(ypre16.shape[0]):\n",
    "        if ypre16[j][i] == 0:\n",
    "            vote[0] += 1\n",
    "        elif ypre16[j][i] == 1:\n",
    "            vote[1] += 1\n",
    "        else:\n",
    "            vote[2] +=1\n",
    "    majorityvote16[i] = np.argmax(vote)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score is:  0.616875878447\n",
      "F1 score is:  0.587360808148\n",
      "precision score is:  0.586767538674\n",
      "recall score is:  0.588233053739\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy score is: \",accuracy_score(ytest16, majorityvote16))\n",
    "print(\"F1 score is: \",f1_score(ytest16, majorityvote16, average=\"macro\"))\n",
    "print(\"precision score is: \", precision_score(ytest16, majorityvote16, average=\"macro\"))\n",
    "print(\"recall score is: \", recall_score(ytest16, majorityvote16, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ypre17 = []\n",
    "ypre17.append(log_glove_pre17)\n",
    "ypre17.append(svc_glove_pre17)\n",
    "ypre17.append(nb_glove_pre17)\n",
    "ypre17.append(log_w2v_pre17)\n",
    "ypre17.append(svc_w2v_pre17)\n",
    "ypre17.append(nb_w2v_pre17)\n",
    "ypre17.append(log_bow_pre17)\n",
    "ypre17.append(svc_bow_pre17)\n",
    "ypre17.append(nb_bow_pre17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ypre17 = np.asarray(ypre17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "majorityvote17 = np.zeros(ypre17.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(ypre17.shape[1]):\n",
    "    vote = np.zeros(3)\n",
    "    for j in range(ypre17.shape[0]):\n",
    "        if ypre17[j][i] == 0:\n",
    "            vote[0] += 1\n",
    "        elif ypre17[j][i] == 1:\n",
    "            vote[1] += 1\n",
    "        else:\n",
    "            vote[2] +=1\n",
    "    majorityvote17[i] = np.argmax(vote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score is:  0.559182676653\n",
      "F1 score is:  0.487125892223\n",
      "precision score is:  0.500565656708\n",
      "recall score is:  0.554786573987\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy score is: \",accuracy_score(ytest17, majorityvote17))\n",
    "print(\"F1 score is: \",f1_score(ytest17, majorityvote17, average=\"macro\"))\n",
    "print(\"precision score is: \", precision_score(ytest17, majorityvote17, average=\"macro\"))\n",
    "print(\"recall score is: \", recall_score(ytest17, majorityvote17, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
